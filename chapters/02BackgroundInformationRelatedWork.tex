\chapter{Related Work}
\label{chp:02bgwork}

Image fusion algorithms can be categorized based on several factors, including whether they employ learning methods or hand-crafted steps, based on predefined loss functions, and whether labeled datasets are involved in the process. Learning-based methods involve the use of machine learning techniques, such as CNN, GAN, Transformers, Auto-encoders, to learn the features and relationships between input images. In contrast, hand-crafted approaches involve the manual selection and design of specific features and fusion rules.Whether a methods is end-to-end is defined as do they require one or more handcrafted steps. Loss functions are commonly used in learning-based methods to measure the quality of the fused image and to guide the training process. Based on the loss function, the process can be classified as self-supervised, supervised or unsupervised. Finally, labeled datasets can be used to train learning-based methods, where the some form of label is available and ground-truth labels are known, or for evaluating the performance of fusion algorithms. By considering these factors, researchers can select the most appropriate image fusion algorithm for their specific application requirements.

The available VIF methods can be categorized into two groups: traditional methods, which were widely used before the advent of AI, and learning-based methods. Regardless of the classification, all methods consist of three main components: image feature extraction, fusion of features from multiple images, and reconstruction of the image from the fused features. During the feature extraction stage, features are extracted from multiple images. In the fusion stage, the extracted features are compared, and complementary features are incorporated into a single feature map or set. Finally, in the reconstruction stage, the image is reconstructed from the fused set of features. All related studies aim to improve one or more of these stages in the process.

Learning-based fusion methods are often used to combine information from multiple sources to obtain more accurate and informative results. These methods can be categorized based on whether they require ground truth labels or not. The ground truth annotations don't have to be a form of fused image. If ground truth labels are needed, they are known as supervised methods. On the other hand, if no ground truth labels are required, they are referred to as unsupervised or self-supervised methods. These fusion techniques are commonly employed in convolutional-based methods, which are widely used in image processing and computer vision applications. Regardless of the specific method used, the goal of learning-based fusion methods is to improve the quality and accuracy of the resulting output by leveraging information from multiple sources.

Learning-based image fusion algorithms are commonly categorized based on the type of network used in the algorithm. There are several types of networks that are frequently used, including CNN-based, auto-encoder based, GAN-based, transformer-based networks and others which include hybrid or handcrafted steps. Convolutional neural network (CNN)-based methods are often used due to their ability to extract features from input images and produce high-quality results. Auto-encoder-based methods are also popular, as they can effectively compress and decompress image information to obtain a fused image. Generative adversarial network (GAN)-based methods use a combination of generator and discriminator networks to produce high-quality fused images. Transformer-based methods have recently gained popularity due to their ability to process long sequences of input data efficiently. Other methods, such as sparse representation-based methods and wavelet-based methods, are also used in learning-based image fusion. Ultimately, the choice of network depends on the specific requirements of the task at hand and the available resources.

In addition to categorizing fusion algorithms based on the type of network used, another way to differentiate them is whether they are end-to-end. An end-to-end algorithm is one that can take raw input data and produce the desired output directly, without any intermediate hand-crafted steps. In the context of image fusion, end-to-end algorithms are those that can take multiple input images and produce a fused image without the need for manual feature extraction or other preprocessing steps. These algorithms are often preferred, as they can be more efficient and less prone to errors compared to non-end-to-end methods that require manual intervention. In contrast, non-end-to-end algorithms may require additional processing steps, such as registration and feature extraction, to produce the final fused image. While these steps can provide additional control over the fusion process, they can also introduce additional complexity and reduce the overall efficiency of the algorithm.

\section{Traditional Algorithms}

The traditional image fusion algorithms have been extensively studied in the literature. However, they are not without shortcomings. One of the major issues with these methods is the presence of handcrafted steps, which can lead to suboptimal results. In addition, the time complexity of some methods can be quite high, making them impractical for real-world applications.

Sparse representation (SR)\cite[]{liu2017infrared}  based methods are a popular choice for image fusion. However, they suffer from several limitations. For example, methods such as \cite{bin2016efficient} and \cite{zhang2013dictionary} require dictionary learning, which can significantly increase the time complexity of the algorithm. Furthermore, these methods include handcrafted steps, which can limit their generalizability.

Another commonly used approach for image fusion is multi-scale transformation (MST) based methods. These methods, such as \cite{hu2017adaptive} and \cite{he2017infrared}, can be quite effective at capturing various characteristics of images at different scales. However, they too suffer from limitations. One major issue is their lack of generalizability, which can make them less effective in certain scenarios.

Low-rank representation (LRR) based methods are another popular choice for image fusion. These methods, such as \cite{text}{liu2012robust}, are particularly effective at dealing with noise and other forms of image degradation. However, like the other methods, they too have limitations. For example, they may not be suitable for all types of images, particularly those with complex textures or patterns.

In summary, the success of traditional image fusion algorithms heavily relies on the quality of the feature extraction method used. While there are many different methods available, each with its own strengths and weaknesses, it is important to carefully consider the specific requirements of the problem at hand before selecting a particular method.

\section{CNN Based Deep Learning Algorithms}


First image fusion method that utilizes CNN is created by Liu et al \cite{liu2018infrared}. The paper proposed a method to fuse infrared and visible images using Convolutional Neural Networks (CNNs). The approach consisted of four main steps: preprocessing, feature extraction, fusion strategy, and reconstruction. First, the input images were resized and normalized. Then, a CNN model was trained to extract features from both the visible and infrared images. The extracted features were then fused using a weighted sum fusion strategy, and the weights were learned during training. Finally, the fused feature maps were transformed back into the image domain using a deconvolutional network. The proposed method was evaluated on various datasets and outperformed other state-of-the-art fusion methods in terms of objective quality metrics and subjective visual quality. The proposed method in the paper also utilizes a multiscale approach to fuse the source images. The images are decomposed into different scales using Laplacian and Gaussian pyramids. This allows the model to better preserve details in both the visible and infrared images during the fusion process. To train the model, high-quality images and their blurred versions are used. These blurred versions are generated by applying multiscale Gaussian filtering and random sampling, which helps the model to learn features that are robust to image blurring and noise. It's also important to note that the proposed method is a pioneering deep learning-based approach to VIF, which is a metric used to evaluate the visual quality of the fused images. By introducing CNNs to VIF, the proposed method achieved better fusion results than other state-of-the-art methods available at that time. 

CNN-based methods can be categorized into two groups: supervised and unsupervised methods. While both methods use CNNs to extract features from the input images, supervised methods require labeled data during the training process, whereas unsupervised methods do not. The majority of CNN-based image fusion methods are unsupervised, as labeled data is often difficult or expensive to obtain. However, to improve the performance of supervised methods, extra steps, such as data augmentation or transfer learning, are often employed. Despite the success of both supervised and unsupervised CNN-based methods, there is still room for improvement in image fusion techniques, particularly in scenarios where the input images have significant differences in terms of illumination, resolution, or noise.

\subsection{Supervised CNNs}
\label{subsec:supervised}

Image fusion is a challenging task, as obtaining ground truth data is often difficult or impossible. However, there are several workarounds that have been proposed to generate pseudo annotations and ground truth data. One approach is to use the results of another fusion method as ground truth data. This can be done by comparing the output of the method being evaluated to the output of a known or well-established fusion method. Another approach is to generate blurred versions of the high-quality input images and use these blurred images as labels. For example, in the study by Liu et al. \cite{liu2018infrared}, blurred versions of the high-quality input images were used to train a deep learning-based fusion model. Similarly, in the study by An et al. \cite{an2020infrared}, the results of other fusion methods were used as annotations to train a convolutional neural network for image fusion. While these approaches are not a substitute for true ground truth data, they provide a means to evaluate and improve image fusion methods, especially in scenarios where obtaining actual ground truth is challenging or impossible.

It is also wise to state that many CNN-based image fusion methods rely on transfer learning, where pre-trained models, such as ResNet50 \cite{li2019infrared,li2021infrared}, VGG19\cite{li2018infrared,ren2018infrared}, VGG16 \cite{yang2021vmdm}, and DenseNet-201 \cite{li2020unsupervised}, are used to extract features from source images. The extracted features are then processed before being fused using manually designed rules, such as weighted averaging or combining the fused base and detail parts. Some studies generate weight maps based on the extracted features \cite{li2019infrared,li2020unsupervised,yang2021vmdm}, while others optimize a loss function computed based on the extracted features using the L-BFGS method \cite{ren2018infrared}. These transfer learning-based methods provide a means to leverage pre-existing knowledge in large-scale datasets and improve the performance of image fusion methods.

\subsection{Unsupervised CNNs}
\label{subsec:unsupervised}

In the field of infrared visual image fusion, most related studies are unsupervised. Due to the lack of a ground truth for this task, loss functions are typically defined as a function of source images and related evaluation metrics. This means that the performance of the fusion method is evaluated based on how well the fused image aligns with the intended objective, rather than how closely it matches a predefined target. While this approach can be challenging due to the subjective nature of image quality evaluation, it remains a popular method for evaluating unsupervised image fusion techniques.

Several studies in the field of image fusion have explored the use of unsupervised convolutional neural networks (CNNs) to improve the process. These CNNs can be applied to either a single part of the process or the entire process. For instance, Liu et al. \cite{liu2019infrared} decompose source images into a base and a detail parts using CNNs, while Hou et al. \cite{hou2020vif} uses CNNs in the feature extraction and reconstruction parts of the process. Some studies, such as Xu et al. \cite{xu2022cufd} and Mustafa et al. \cite{mustafa2020infrared}, use unsupervised CNNs for the entire image fusion process. By utilizing unsupervised CNNs, these studies aim to improve the overall performance of the image fusion process, particularly for tasks such as infrared visual image fusion.

In addition to unsupervised CNNs, there are other methods that can be used to improve the performance of infrared and visual image fusion. Residual connections \cite{he2016deep} and dense connections \cite{huang2017densely} are examples of techniques that have been used to enhance feature propagation within neural networks. Attention mechanisms \cite{mnih2014recurrent} have also been applied to focus on important features and exclude irrelevant ones, while multi-scale and multilevel features have been used to capture details across different spatial and frequency ranges. Contrastive learning \cite{hinton2006dimensionality} and neural architecture search \cite{zoph2017neural} are other approaches that have been used to improve the performance of image fusion methods. Image and feature decomposition techniques can also be used to decompose the source images into different components and extract features from them. By combining these various methods, researchers can develop more sophisticated image fusion systems that produce higher-quality results.

\section{Autoencoder Based Deep Learning Algorithms}

Autoencoder is a type of neural network that is widely utilized for unsupervised learning tasks, including but not limited to dimensionality reduction, data compression, and anomaly detection. The fundamental principle of autoencoder is to compress the input data into a lower-dimensional representation, which is commonly referred to as the latent space. The compressed representation is then utilized to reconstruct the original input data. The objective is to minimize the difference between the input data and the reconstructed output, which requires the autoencoder to learn a compressed representation that captures the most essential features of the input data. The concept of autoencoder was first introduced by Hinton et al.\cite{hinton2006reducing}.

In the context of infrared visual image fusion, the autoencoder technique is leveraged to extract features from source images using the encoder stage, while the decoder stage reconstructs the fused image. The training process typically involves two stages: firstly, the autoencoder is trained using source images, either infrared, visual or both, without any fusion. Subsequently, the fusion step is integrated and the entire model is trained. It is also common practice to employ large datasets for the first training stage. It is worth noting that the autoencoder approach, which is used in this method, differs from transfer learning methods discussed in Section \ref{subsec:supervised} in that an autoencoder is trained from scratch, while pre-trained models are utilized in transfer learning with minimal fine tuning. One of the pioneering works in infrared and visual image fusion using autoencoder is DenseFuse \cite{li2019infrared}. To pre-train the network without the fusion step, the well-known MS-COCO dataset \cite{lin2014microsoft} is utilized in the first stage training. Other noteworthy studies include Raza et al. \cite{raza2020pfaf}, Fu et al. \cite{fu2021dual}, Jian et al. \cite{jian2020sedrfuse}, Wang et al. \cite{wang2022res2fusion}, and finally, Zhao et al. \cite{zhao2021efficient}.

There are still open research questions for this part since rgb and infrared images are different in structure and only rgb image is used in autoencoder's training. There are studies that focuses on this difference. IVFENet \cite{zhao2021self}, an encoder and two decoders are used in the pretraining stage to alleviate the vital information loss. Liu et al. \cite{liu2021smoa} use two encoders and one unified decoder, which utilizes a NAS technique for the model. Measures for performance increase in Section \ref{subsec:unsupervised} are also used with autoencoder based models such as dense connections \cite{pan2021densenetfuse, wang2021unfusion, li2022infrared}, attention mechanism \cite{wang2021unfusion}, multiscale features \cite{li2019infrared,wang2021unfusion}, multi-level features \cite{peng2022mfdetection}.

\section{GAN Based Deep Learning Algorithms}

Since its introduction by Goodfellow et al. \cite{goodfellow2014generative} in 2014, General Adversarial Networks, also known as GAN, have found a wide range of applications. In 2019, Ma et al. \cite{ma2019fusiongan} introduced GAN to the image fusion task, after which many GAN-based image fusion methods have been proposed. Typically, these methods are unsupervised and utilize various combinations of one-generator-to-one-discriminator, one-generator-to-more-discriminators, and two-generators-to-two-discriminators. Additionally, some supervised versions of GAN-based image fusion methods have been developed.

\subsection{Unsupervised GANs}

Most of the GAN-based VIF methods are unsupervised methods, which means that they do not require labeled data for training. Instead, the training process is driven by a loss function that measures the difference between the fused image and the source images. This loss function typically contains several terms that reflect the difference from different perspectives. For example, one term may measure the pixel-wise difference between the fused image and the source images, while another term may measure the structural similarity between the fused image and the source images.

In addition to the loss function, evaluation metrics are also used to assess the quality of the fused image. These metrics provide quantitative measures of various aspects of the fused image, such as its spatial frequency, contrast, and sharpness. Some commonly used evaluation metrics for VIF include mutual information, entropy, edge preservation, and spatial frequency. The choice of evaluation metrics can depend on the specific application of the VIF method.

Overall, the unsupervised nature of GAN-based VIF methods allows them to learn from data without the need for manual labeling, making them flexible and adaptable to a wide range of applications. The use of loss functions and evaluation metrics further enhances the performance and effectiveness of these methods, enabling them to produce high-quality fused images that preserve the most important features of the source images.

\paragraph{One Generator to One Discriminator}

Ma et al. \cite{ma2019fusiongan} introduced the FusionGAN as the pioneer GAN-based VIF method. The method involves a generator that produces a fused image with emphasized targets, while the discriminator's role is to enforce the fused image to contain more textural details, similar to those found in visible images. The authors evaluated the effectiveness of FusionGAN on several datasets, including multi-spectral, hyperspectral, and infrared images. The results showed that FusionGAN outperformed existing state-of-the-art  (SoTA) VIF methods in terms of both visual quality and quantitative metrics such as peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Moreover, the proposed method is robust to changes in fusion rules, fusion strategies, and image sizes. These promising results demonstrate the potential of GAN-based approaches for improving VIF performance in various imaging scenarios.

As in Section \ref{subsec:unsupervised} some measures have also
been used to improve fusion performance. For example, Xu
et al. [68] utilized a local binary pattern loss for training. Xu
et al. [65] utilized residual blocks and skip connections in
the generator. Fu et al. [100] proposed to utilize dense blocks
to allow the generator to learn more information. They not
only concatenate the features of shallow layers and deeper
layers but also insert the visible image at each layer of the
generator to help the network learn visible information. Furthermore, other measures, such as attention mechanisms




