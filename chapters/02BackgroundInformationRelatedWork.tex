\chapter{Related Work}
\label{chp:02bgwork}

Image fusion algorithms can be categorized based on several factors, including whether they employ learning methods or hand-crafted steps, based on predefined loss functions, and whether labeled datasets are involved in the process. Learning-based methods involve the use of machine learning techniques, such as CNN, GAN, Transformers, Auto-encoders, to learn the features and relationships between input images. In contrast, hand-crafted approaches involve the manual selection and design of specific features and fusion rules.Whether a methods is end-to-end is defined as do they require one or more handcrafted steps. Loss functions are commonly used in learning-based methods to measure the quality of the fused image and to guide the training process. Based on the loss function, the process can be classified as self-supervised, supervised or unsupervised. Finally, labeled datasets can be used to train learning-based methods, where the some form of label is available and ground-truth labels are known, or for evaluating the performance of fusion algorithms. By considering these factors, researchers can select the most appropriate image fusion algorithm for their specific application requirements.

The available VIF methods can be categorized into two groups: traditional methods, which were widely used before the advent of AI, and learning-based methods. Regardless of the classification, all methods consist of three main components: image feature extraction, fusion of features from multiple images, and reconstruction of the image from the fused features. During the feature extraction stage, features are extracted from multiple images. In the fusion stage, the extracted features are compared, and complementary features are incorporated into a single feature map or set. Finally, in the reconstruction stage, the image is reconstructed from the fused set of features. All related studies aim to improve one or more of these stages in the process.

Learning-based fusion methods are often used to combine information from multiple sources to obtain more accurate and informative results. These methods can be categorized based on whether they require ground truth labels or not. The ground truth annotations don't have to be a form of fused image. If ground truth labels are needed, they are known as supervised methods. On the other hand, if no ground truth labels are required, they are referred to as unsupervised or self-supervised methods. These fusion techniques are commonly employed in convolutional-based methods, which are widely used in image processing and computer vision applications. Regardless of the specific method used, the goal of learning-based fusion methods is to improve the quality and accuracy of the resulting output by leveraging information from multiple sources.

Learning-based image fusion algorithms are commonly categorized based on the type of network used in the algorithm. There are several types of networks that are frequently used, including CNN-based, auto-encoder based, GAN-based, transformer-based networks and others which include hybrid or handcrafted steps. Convolutional neural network (CNN)-based methods are often used due to their ability to extract features from input images and produce high-quality results. Auto-encoder-based methods are also popular, as they can effectively compress and decompress image information to obtain a fused image. Generative adversarial network (GAN)-based methods use a combination of generator and discriminator networks to produce high-quality fused images. Transformer-based methods have recently gained popularity due to their ability to process long sequences of input data efficiently. Other methods, such as sparse representation-based methods and wavelet-based methods, are also used in learning-based image fusion. Ultimately, the choice of network depends on the specific requirements of the task at hand and the available resources.

In addition to categorizing fusion algorithms based on the type of network used, another way to differentiate them is whether they are end-to-end. An end-to-end algorithm is one that can take raw input data and produce the desired output directly, without any intermediate hand-crafted steps. In the context of image fusion, end-to-end algorithms are those that can take multiple input images and produce a fused image without the need for manual feature extraction or other preprocessing steps. These algorithms are often preferred, as they can be more efficient and less prone to errors compared to non-end-to-end methods that require manual intervention. In contrast, non-end-to-end algorithms may require additional processing steps, such as registration and feature extraction, to produce the final fused image. While these steps can provide additional control over the fusion process, they can also introduce additional complexity and reduce the overall efficiency of the algorithm.

\section{Traditional Fusion Algorithms}

The traditional image fusion algorithms have been extensively studied in the literature. However, they are not without shortcomings. One of the major issues with these methods is the presence of handcrafted steps, which can lead to suboptimal results. In addition, the time complexity of some methods can be quite high, making them impractical for real-world applications.

Sparse representation (SR)\cite{liu2017infrared}  based methods are a popular choice for image fusion. However, they suffer from several limitations. For example, methods such as \cite{bin2016efficient} and \cite{zhang2013dictionary} require dictionary learning, which can significantly increase the time complexity of the algorithm. Furthermore, these methods include handcrafted steps, which can limit their generalizability.

Another commonly used approach for image fusion is multi-scale transformation (MST) based methods. These methods, such as \cite{hu2017adaptive} and \cite{he2017infrared}, can be quite effective at capturing various characteristics of images at different scales. However, they too suffer from limitations. One major issue is their lack of generalizability, which can make them less effective in certain scenarios.

Low-rank representation (LRR) based methods are another popular choice for image fusion. These methods, such as \cite{liu2012robust}, are particularly effective at dealing with noise and other forms of image degradation. However, like the other methods, they too have limitations. For example, they may not be suitable for all types of images, particularly those with complex textures or patterns.

In summary, the success of traditional image fusion algorithms heavily relies on the quality of the feature extraction method used. While there are many different methods available, each with its own strengths and weaknesses, it is important to carefully consider the specific requirements of the problem at hand before selecting a particular method.

\section{CNN Based Deep Learning Algorithms}
\label{sec:CNN}

First image fusion method that utilizes CNN is created by Liu et al \cite{liu2018infrared}. The paper proposed a method to fuse infrared and visible images using Convolutional Neural Networks (CNNs). The approach consisted of four main steps: preprocessing, feature extraction, fusion strategy, and reconstruction. First, the input images were resized and normalized. Then, a CNN model was trained to extract features from both the visible and infrared images. The extracted features were then fused using a weighted sum fusion strategy, and the weights were learned during training. Finally, the fused feature maps were transformed back into the image domain using a deconvolutional network. The proposed method was evaluated on various datasets and outperformed other state-of-the-art fusion methods in terms of objective quality metrics and subjective visual quality. The proposed method in the paper also utilizes a multiscale approach to fuse the source images. The images are decomposed into different scales using Laplacian and Gaussian pyramids. This allows the model to better preserve details in both the visible and infrared images during the fusion process. To train the model, high-quality images and their blurred versions are used. These blurred versions are generated by applying multiscale Gaussian filtering and random sampling, which helps the model to learn features that are robust to image blurring and noise. It's also important to note that the proposed method is a pioneering deep learning-based approach to VIF, which is a metric used to evaluate the visual quality of the fused images. By introducing CNNs to VIF, the proposed method achieved better fusion results than other state-of-the-art methods available at that time. 

CNN-based methods can be categorized into two groups: supervised and unsupervised methods. While both methods use CNNs to extract features from the input images, supervised methods require labeled data during the training process, whereas unsupervised methods do not. The majority of CNN-based image fusion methods are unsupervised, as labeled data is often difficult or expensive to obtain. However, to improve the performance of supervised methods, extra steps, such as data augmentation or transfer learning, are often employed. Despite the success of both supervised and unsupervised CNN-based methods, there is still room for improvement in image fusion techniques, particularly in scenarios where the input images have significant differences in terms of illumination, resolution, or noise.

\subsection{Supervised CNNs}
\label{subsec:supervised}

Image fusion is a challenging task, as obtaining ground truth data is often difficult or impossible. However, there are several workarounds that have been proposed to generate pseudo annotations and ground truth data. One approach is to use the results of another fusion method as ground truth data. This can be done by comparing the output of the method being evaluated to the output of a known or well-established fusion method. Another approach is to generate blurred versions of the high-quality input images and use these blurred images as labels. For example, in the study by Liu et al. \cite{liu2018infrared}, blurred versions of the high-quality input images were used to train a deep learning-based fusion model. Similarly, in the study by An et al. \cite{an2020infrared}, the results of other fusion methods were used as annotations to train a convolutional neural network for image fusion. While these approaches are not a substitute for true ground truth data, they provide a means to evaluate and improve image fusion methods, especially in scenarios where obtaining actual ground truth is challenging or impossible.

It is also wise to state that many CNN-based image fusion methods rely on transfer learning, where pre-trained models, such as ResNet50 \cite{li2019infrared,li2021infrared}, VGG19\cite{li2018infrared,ren2018infrared}, VGG16 \cite{yang2021vmdm}, and DenseNet-201 \cite{li2020unsupervised}, are used to extract features from source images. The extracted features are then processed before being fused using manually designed rules, such as weighted averaging or combining the fused base and detail parts. Some studies generate weight maps based on the extracted features \cite{li2019infrared,li2020unsupervised,yang2021vmdm}, while others optimize a loss function computed based on the extracted features using the L-BFGS method \cite{ren2018infrared}. These transfer learning-based methods provide a means to leverage pre-existing knowledge in large-scale datasets and improve the performance of image fusion methods.

\subsection{Unsupervised CNNs}
\label{subsec:unsupervised}

In the field of infrared visual image fusion, most related studies are unsupervised. Due to the lack of a ground truth for this task, loss functions are typically defined as a function of source images and related evaluation metrics. This means that the performance of the fusion method is evaluated based on how well the fused image aligns with the intended objective, rather than how closely it matches a predefined target. While this approach can be challenging due to the subjective nature of image quality evaluation, it remains a popular method for evaluating unsupervised image fusion techniques.

Several studies in the field of image fusion have explored the use of unsupervised convolutional neural networks (CNNs) to improve the process. These CNNs can be applied to either a single part of the process or the entire process. For instance, Liu et al. \cite{liu2019infrared} decompose source images into a base and a detail parts using CNNs, while Hou et al. \cite{hou2020vif} uses CNNs in the feature extraction and reconstruction parts of the process. Some studies, such as Xu et al. \cite{xu2022cufd} and Mustafa et al. \cite{mustafa2020infrared}, use unsupervised CNNs for the entire image fusion process. By utilizing unsupervised CNNs, these studies aim to improve the overall performance of the image fusion process, particularly for tasks such as infrared visual image fusion.

In addition to unsupervised CNNs, there are other methods that can be used to improve the performance of infrared and visual image fusion. Residual connections \cite{he2016deep} and dense connections \cite{huang2017densely} are examples of techniques that have been used to enhance feature propagation within neural networks. Attention mechanisms \cite{mnih2014recurrent} have also been applied to focus on important features and exclude irrelevant ones, while multi-scale and multilevel features have been used to capture details across different spatial and frequency ranges. Contrastive learning \cite{hinton2006dimensionality} and neural architecture search \cite{zoph2017neural} are other approaches that have been used to improve the performance of image fusion methods. Image and feature decomposition techniques can also be used to decompose the source images into different components and extract features from them. By combining these various methods, researchers can develop more sophisticated image fusion systems that produce higher-quality results.

\section{Autoencoder Based Deep Learning Algorithms}
\label{sec:AE}

Autoencoder is a type of neural network that is widely utilized for unsupervised learning tasks, including but not limited to dimensionality reduction, data compression, and anomaly detection. The fundamental principle of autoencoder is to compress the input data into a lower-dimensional representation, which is commonly referred to as the latent space. The compressed representation is then utilized to reconstruct the original input data. The objective is to minimize the difference between the input data and the reconstructed output, which requires the autoencoder to learn a compressed representation that captures the most essential features of the input data. The concept of autoencoder was first introduced by Hinton et al.\cite{hinton2006reducing}.

In the context of infrared visual image fusion, the autoencoder technique is leveraged to extract features from source images using the encoder stage, while the decoder stage reconstructs the fused image. The training process typically involves two stages: firstly, the autoencoder is trained using source images, either infrared, visual or both, without any fusion. Subsequently, the fusion step is integrated and the entire model is trained. It is also common practice to employ large datasets for the first training stage. It is worth noting that the autoencoder approach, which is used in this method, differs from transfer learning methods discussed in Section \ref{subsec:supervised} in that an autoencoder is trained from scratch, while pre-trained models are utilized in transfer learning with minimal fine tuning. One of the pioneering works in infrared and visual image fusion using autoencoder is DenseFuse \cite{li2019infrared}. To pre-train the network without the fusion step, the well-known MS-COCO dataset \cite{lin2014microsoft} is utilized in the first stage training. Other noteworthy studies include Raza et al. \cite{raza2020pfaf}, Fu et al. \cite{fu2021dual}, Jian et al. \cite{jian2020sedrfuse}, Wang et al. \cite{wang2022res2fusion}, and finally, Zhao et al. \cite{zhao2021efficient}.

There are still open research questions for this part since rgb and infrared images are different in structure and only rgb image is used in autoencoder's training. There are studies that focuses on this difference. IVFENet \cite{zhao2021self}, an encoder and two decoders are used in the pretraining stage to alleviate the vital information loss. Liu et al. \cite{liu2021smoa} use two encoders and one unified decoder, which utilizes a NAS technique for the model. Measures for performance increase in Section \ref{subsec:unsupervised} are also used with autoencoder based models such as dense connections \cite{pan2021densenetfuse, wang2021unfusion, li2022infrared}, attention mechanism \cite{wang2021unfusion}, multiscale features \cite{li2019infrared,wang2021unfusion}, multi-level features \cite{peng2022mfdetection}.

\section{GAN Based Deep Learning Algorithms}

Since its introduction by Goodfellow et al. \cite{goodfellow2014generative} in 2014, General Adversarial Networks, also known as GAN, have found a wide range of applications. In 2019, Ma et al. \cite{ma2019fusiongan} introduced GAN to the image fusion task, after which many GAN-based image fusion methods have been proposed. Typically, these methods are unsupervised and utilize various combinations of one-generator-to-one-discriminator, one-generator-to-more-discriminators, and two-generators-to-two-discriminators. Additionally, some supervised versions of GAN-based image fusion methods have been developed.

\subsection{Unsupervised GANs}

Most of the GAN-based VIF methods are unsupervised methods, which means that they do not require labeled data for training. Instead, the training process is driven by a loss function that measures the difference between the fused image and the source images. This loss function typically contains several terms that reflect the difference from different perspectives. For example, one term may measure the pixel-wise difference between the fused image and the source images, while another term may measure the structural similarity between the fused image and the source images.

In addition to the loss function, evaluation metrics are also used to assess the quality of the fused image. These metrics provide quantitative measures of various aspects of the fused image, such as its spatial frequency, contrast, and sharpness. Some commonly used evaluation metrics for VIF include mutual information, entropy, edge preservation, and spatial frequency. The choice of evaluation metrics can depend on the specific application of the VIF method.

Overall, the unsupervised nature of GAN-based VIF methods allows them to learn from data without the need for manual labeling, making them flexible and adaptable to a wide range of applications. The use of loss functions and evaluation metrics further enhances the performance and effectiveness of these methods, enabling them to produce high-quality fused images that preserve the most important features of the source images.

In Section \ref{subsec:unsupervised}, researchers have explored various methods to enhance the performance of fusion techniques. Numerous approaches have been proposed in the literature to improve the fusion process and achieve better results. For instance, in the work of Xu et al. \cite{xu2020lbp}, they incorporated a local binary pattern loss during the training phase. This loss function allowed the model to capture local texture information, leading to improved fusion outcomes. Another study by Xu et al. \cite{xu2020infrared} introduced the use of residual blocks and skip connections in the generator. By leveraging these architectural components, the model was able to learn residual features and establish direct connections between different layers, facilitating the flow of information. This architectural design helped to alleviate the vanishing gradient problem and enhanced the overall fusion performance.

In a different approach, Fu et al. \cite{fu2021image} proposed the utilization of dense blocks to further augment the capabilities of the generator. Dense blocks enabled the model to learn richer feature representations by densely connecting layers within the generator. This architecture not only encouraged the fusion of features from different depths but also facilitated the learning of intricate patterns and relationships within the input data. To leverage the valuable information contained in the visible image, Fu et al. \cite{fu2021image} took an innovative step by incorporating the visible image at each layer of the generator. This strategy enabled the network to effectively capture visible information and incorporate it into the fusion process. By fusing features from the visible image with those learned from the input images, the model was able to enhance the visibility of important details and improve the overall quality of the fused image. Furthermore, attention mechanisms \cite{wang2021new} and residual connections \cite{xu2020infrared} have also been employed as supplementary techniques in fusion methods. Attention mechanisms allow the model to focus on informative regions and weight their contributions accordingly, while residual connections facilitate the flow of gradients and aid in training deep networks. These techniques have proven to be effective in enhancing fusion performance and achieving state-of-the-art results in various fusion tasks. By integrating these innovative approaches and techniques, researchers aim to overcome the challenges of information fusion and improve the quality and effectiveness of fusion models. These advancements pave the way for more accurate and visually appealing fused images in a wide range of applications, such as remote sensing, medical imaging, and computer vision.

The previously mentioned methods utilize original visible and infrared images as inputs for the generator. However, these approaches only focus on the primary information found in each modality and neglect the supplementary information. Consequently, the resulting fused images tend to resemble sharpened infrared images. To tackle this challenge, Ma et al. introduced the GANMcC method \cite{ma2020ganmcc}. GANMcC incorporates a two-branch architecture within the generator, where each branch (gradient and contrast) employs different combinations of source images as input. This design allows the generator to capture both the main and auxiliary information. Another notable method is MFEIF \cite{liu2021learning}, which does not require well-aligned image pairs during training. MFEIF utilizes a coarse-to-fine deep architecture to leverage multiscale features and integrates a cross-domain edge-guided attention mechanism, which directs the model's focus towards common structures to preserve finer details. Moreover, Liao et al. proposed a technique \cite{liao2020fusion} that employs VGG19 to extract features from both the visible and fused images generated by the generator. They then minimize the Wasserstein distance within the feature space. A major drawback of most GAN-based methods is their reliance on a single discriminator to enforce similarity either to the visible or infrared image. This approach often leads to the loss of details from the other modality during the adversarial training process. To overcome this limitation, Ma et al. \cite{ma2020ganmcc} introduced a multiclassification-based discriminator, aiming to strike a balance between the visible and infrared distribution. Some researchers have also proposed the use of multiple discriminators as a potential solution to this issue.

To address the limitations of considering a single source image in the discriminator, researchers have extended GAN-based methods to incorporate two or more discriminators, aiming to preserve features from both source images. For instance, Xu et al. proposed DDcGAN, a VIF method with two discriminators that can fuse source images of different resolutions \cite{xu2019learning}. Ma et al. further extended this method by using a densely connected CNN in the generator, taking the image itself as input in the discriminator, and employing a deconvolution layer for upsampling the infrared image \cite{ma2020ddcgan}. Other researchers also recognized the benefits of employing multiple discriminators. Li et al. developed unsupervised GAN-based VIF methods with one generator and two discriminators, such as MD-WGAN, D2WGAN, and MgAN-Fuse \cite{li2019infrared, li2020infrared, li2020multigrained}. These methods introduced various improvements, including the use of texture loss, Wasserstein distance, multiscale attention, and separate encoders for visible and infrared images. Li et al. also proposed AttentionFGAN, incorporating multiscale attention mechanisms in the generator and discriminators \cite{li2020attentionfgan}. Additionally, Zhang et al. utilized a full-scale skip connection-based generator and two Markovian discriminators to retain useful information from both visible and infrared source images \cite{zhang2021gan}. Song et al. recently introduced a VIF method with one generator and three discriminators, including a difference image discriminator \cite{song2022triple}. These advancements involving multiple discriminators aim to enhance the fusion process and consider various aspects of the input images.

\subsection{Supervised GANs}

In the context of supervised GAN-based methods, similar to the supervised CNN methods discussed in Section \ref{subsec:supervised}, there are several approaches that utilize different types of ground truth. The first type involves using fused images generated by other methods as the reference. Lebedev et al. \cite{lebedev2019multisensor} proposed a method with one generator and one discriminator, where fused images obtained through the Laplacian pyramid algorithm accompanied by MultiScale Retinex served as the ground truth. Another approach, RCGAN, introduced by Li et al.\cite{li2019coupled}, is based on coupled GAN and features two generators and two discriminators. Notably, RCGAN incorporates pre-fused images generated by GFF \cite{li2013image} for optimization within the coupled generators. However, it is important to consider that the performance of RCGAN can be influenced by the specific method used to generate the pre-fused images.

\section{Transformer Based Deep Learning Algorithms}

Transformers have emerged as versatile tools capable of handling long-range dependencies in diverse domains, including natural language processing and computer vision tasks \cite{dosovitskiy2020image, liu2021swin, liu2022mfst}. Their entry into the realm of image fusion in 2021 has opened up exciting prospects for advancing image fusion techniques such as \cite{zhao2021dndt, rao2023tgfuse, li2022cgtf, tang2022ydtr, wang2022swinfuse, yang2023dglt, tang2023tccfusion}. By harnessing the capabilities of transformers, a range of transformer-based methods have been developed to enhance various aspects of image fusion, encompassing both general fusion scenarios \cite{vs2022image}, \cite{fu2021ppt}, \cite{ma2022swinfusion}, \cite{qu2022transfuse}. These methods capitalize on transformers' inherent ability to capture contextual relationships and dependencies across different regions within images, enabling a more comprehensive fusion of visual information. Leveraging the self-attention mechanism, these methods effectively model interactions between pixels or patches, facilitating the fusion of relevant features while preserving crucial details. In short, the integration of transformers into the field of image fusion has sparked notable advancements, empowering the creation of transformer-based methods for image fusion applications. Leveraging transformers' prowess in capturing long-range dependencies and contextual information, these methods pave the way for more sophisticated and accurate fusion of visual data, elevating the possibilities in the field of image fusion.

In the realm of feature fusion, certain methods leverage the transformer architecture for this purpose. One approach, presented by VS et al. \cite{vs2022image}, utilizes a transformer-based multiscale fusion strategy to merge local and global information effectively. This method acknowledges the importance of incorporating both local and global context in the fusion process. Similarly, Zhao et al. \cite{zhao2021dndt} propose DNDT, a method that employs a dual transformer framework for fusion. Their approach involves an encoder to extract features from source images and a decoder to construct the fused image, allowing for the integration of information from multiple sources.

Another line of research focuses on transformer-based fusion blocks. Liu et al. \cite{liu2022mfst} introduce a transformer fusion block that utilizes focal self-attention to fuse multiscale features obtained from a multiscale encoder network. This fusion approach demonstrates the effectiveness of transformer-based techniques in incorporating multiscale information. Recent advancements have extended the application of transformers to image fusion tasks. Rao et al. \cite{rao2023tgfuse} propose a VIF method that combines transformers and GAN. Their approach integrates a spatial transformer and a channel transformer within the generator, enabling the fusion of spatial and channel-wise information. Additionally, Ma et al. introduce SwinFusion \cite{ma2022swinfusion}, a general image fusion method based on the Swin Transformer. This approach highlights the significance of global information in image fusion and provides valuable visualizations to better understand its impact. Collectively, these transformer-based methods provide innovative solutions for feature fusion in image fusion tasks. They leverage the strengths of transformers to effectively incorporate information from multiple sources and emphasize the importance of global context in the fusion process. By harnessing the capabilities of transformers, these methods contribute to advancements in image fusion techniques and offer new insights into the significance of global information.


In alternative approaches, transformers are not only utilized for feature fusion but also play a role in other stages of VIF methods. For instance, Fu et al.\cite{fu2021ppt} introduced a pyramid patch transformer method, which incorporates a transformer-based feature extraction module and an MLP-based decoder within an autoencoder framework. The fusion process in this method follows an average strategy. Similarly, Wang et al. \cite{wang2022swinfuse} developed SwinFuse, an autoencoder-based framework where a transformer-based encoder is employed to extract global features. Tang et al. \cite{tang2022ydtr} proposed YDTR, which combines CNN and transformers in both the encoding and decoding branches. Visible and infrared image features are merged together. Another example is CGTF \cite{wang2022unsupervised}, which utilizes transformer and convolution feature extraction modules in both the encoding and decoding branches. Additionally, Tang et al. \cite{tang2023tccfusion} designed a parallel transformer-based global feature extraction branch alongside their CNN-based local feature extraction branch. Yang et al. \cite{yang2023dglt} employed a combination of transformer blocks and convolution blocks to generate fused images from source images.

The architectural diversity among transformer-based methods reflects the flexibility and adaptability of these approaches in addressing various image fusion challenges. Researchers have explored different ways to integrate transformers and CNNs within the fusion pipeline to capitalize on their respective strengths. By combining the local and global modeling capabilities of CNNs and transformers, these methods aim to achieve a more comprehensive and informative fusion of visual data. 

It is noteworthy that existing transformer-based VIF methods operate in an unsupervised manner. This means that they do not rely on explicit ground truth labels during training. Instead, the loss function is derived from the comparison between the fused image and the source images themselves. This unsupervised nature presents both advantages and challenges. On one hand, it enables the methods to be applicable to a wide range of scenarios without the need for annotated training data. On the other hand, it poses difficulties in evaluating and objectively comparing different methods, as there is no direct reference for the quality of the fusion output. As researchers delve deeper into transformer-based image fusion, there is ongoing exploration of novel architectural designs and techniques. This includes investigating the integration of transformers at different stages of the fusion pipeline, exploring variations in the combination of transformers and CNNs, and exploring the potential of leveraging additional information or guidance to further enhance the fusion process. By pushing the boundaries of transformer-based methods and continually refining their designs, researchers strive to improve the quality and accuracy of fused images, advancing the field of image fusion across diverse domains and applications.

\section{Dataset Usage}

In the image fusion task, the absence of ground truth presents a significant hurdle when it comes to developing supervised VIF methods as discussed in the Section \ref{subsec:supervised} and Section \ref{subsec:unsupervised}. The majority of deep learning-based VIF methods lean towards unsupervised approaches due to the unavailability of reliable ground truth data. However, researchers have not been deterred by this challenge and have made efforts to address it through various means. They have explored alternative methods to generate pseudo ground truth or devised innovative ways of utilizing different forms of labels to facilitate supervised training in the image fusion domain. To tackle the lack of ground truth, researchers have adopted creative strategies to generate pseudo ground truth that approximates the desired fusion result. These approaches often involve using existing fusion methods as a reference to create synthesized fused images, which can then serve as pseudo ground truth during the training process. By utilizing these pseudo ground truth images, supervised fusion methods can be trained in a manner that mimics the desired fusion outcomes, enabling the models to learn and optimize fusion performance based on the generated reference images. In addition to pseudo ground truth generation, researchers have explored alternative forms of labels that can be utilized in supervised training for image fusion. These labels may not directly correspond to ground truth fusion images but can provide valuable information and guidance during the training process. For instance, researchers have employed object masks or segmentation maps as labels, allowing the models to focus on specific regions or objects of interest during the fusion process. By leveraging such labels, supervised VIF methods can be trained to prioritize the preservation of salient features or specific visual elements in the fused images, leading to improved fusion quality and performance. In short, despite the absence of ground truth in the image fusion task, researchers have demonstrated their ingenuity by developing methods that effectively handle the challenge through the use of pseudo ground truth and alternative forms of labels. These approaches enable supervised VIF methods to leverage available information and guidance during the training process, enhancing their ability to learn and optimize fusion outcomes.

Unsupervised VIF methods employ a range of training data strategies to tackle the challenge of lacking supervision. These strategies offer diverse approaches to learn from available data and enhance the performance of image fusion techniques. One approach involves utilizing pairs of visible-infrared images as training data \cite{rao2023tgfuse, xu2021classification, song2022triple}. By leveraging the inherent correlation between these image modalities, the models can learn to extract and fuse relevant features effectively.

Another approach focuses on using independent visible and infrared images, which may not be directly paired, as the training data. This strategy is commonly observed in Autoencoder based methods disccuessed in Section \ref{sec:AE}. These methods leverage the inherent structures within individual modalities to learn meaningful representations and subsequently generate fused images. Although these training data do not possess explicit correspondences, the models aim to capture the underlying patterns and correlations between the modalities.

Additionally, some approaches make use of all-clear visible images, primarily employed in AE-based methods. For example, Li et al. \cite{li2019infrared} utilize the MSCOCO dataset to train the encoder and decoder components. By leveraging the rich visual content present in these images, the models can learn to encode and decode the relevant information necessary for fusion. This data-driven approach benefits from the availability of large-scale visible datasets, enabling the models to capture diverse visual characteristics.

Furthermore, a hybrid strategy combines visible images with visible-infrared image pairs to train different modules of the model. For instance, Jian et al. \cite{jian2021infrared} employ visible images to train an image decomposition module and visible-infrared image pairs to train a stacked sparse autoencoder for local saliency map extraction. This approach leverages the complementary nature of the training data, allowing the model to learn both local saliency and decomposition features for effective fusion.

Lastly, transfer learning emerges as a viable solution to address the scarcity of training data. It involves utilizing pre-trained models that have been trained on large-scale RGB datasets, as mentioned in Section \ref{sec:CNN} By leveraging the knowledge learned from these pre-trained models, the models can acquire generic visual representations that can be adapted and fine-tuned for the task of VIF. This approach offers a practical and effective way to overcome the limitations of limited available training data.

Overall, these diverse training data strategies enable unsupervised VIF methods to learn from different sources of information and exploit inherent correlations, leading to enhanced fusion performance.

\subsection{Methods to Create Labeled Dataset}

Addressing the lack of ground truth in image fusion task is approached through various methods. One method involves using fused images generated by other techniques as a substitute for ground truth. For instance, Li et al. \cite{li2019coupled} utilize the Generative Fusion Framework (GFF) \cite{li2013image} to generate labels, while Lebedev et al.\cite{lebedev2019multisensor} employ the Laplacian pyramid algorithm in combination with MultiScale Retinex \cite{petro2014multiscale} to produce ground truth images. However, this approach may have limitations on the learning process \cite{ma2020ganmcc}.

Another approach involves utilizing clear images and their corresponding blurred versions. This technique has been applied in several studies such as \cite{zhang2020ifcnn,liu2018infrared, wang2019generative, feng2020fully, luo2021ifsepr}, where RGB images and their blurred counterparts are utilized. Recently, Zhu et al. \cite{zhu2022iplf} generated blurred versions for both RGB and infrared images. Nevertheless, the training data produced using this approach may lack realism and differ from actual visible-infrared image pairs.

A different strategy involves utilizing manually-labeled object masks available in existing image fusion datasets. Some researchersm, as in \cite{ma2021stdfusionnet}, use these masks to retain semantic information in the fused images. However, this method is labor-intensive and not always convenient for obtaining the required masks.

Another technique involves transforming the image fusion task, which lacks ground truth, into a task that has ground truth for a specific part of the loss function. This is achieved by incorporating labels from downstream applications. For example, Shopovska et al. \cite{shopovska2019deep} use a pre-trained pedestrian detector to generate pedestrian labels and include these labels as an auxiliary detection loss during training. Tang et al. \cite{tang2022image} employ scene segmentation as a downstream task and integrate a segmentation loss term into the overall loss function to guide training. The scene segmentation labels are manually labeled by the authors of the segmentation dataset. Similarly, Liu et al. \cite{liu2022target} utilize general object detection as a downstream task and introduce an object detection loss term to the loss function, leveraging object detection labels provided by the creator of the detection dataset.

Lastly, the Y channel of RGB images in the YCbCr color space can be used as a form of ground truth. Synthetic infrared and visible images are generated to facilitate training \cite{luo2021latraivf}. In summary, researchers have adopted diverse methods to overcome the absence of ground truth in image fusion tasks. These methods include using fused images from other techniques, clear images and their blurred versions, manually-labeled object masks, labels from downstream applications, or synthetic images. Each approach has its own advantages and considerations, providing alternative means to enable supervised training and enhance the performance of image fusion algorithms.
