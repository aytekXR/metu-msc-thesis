\chapter{Related Work}
\label{chp:02bgwork}

Image fusion algorithms can be categorized based on several factors, including whether they employ learning methods or hand-crafted steps, based on predefined loss functions, and whether labeled datasets are involved in the process. Learning-based methods involve the use of machine learning techniques, such as CNN, GAN, Transformers, Auto-encoders, to learn the features and relationships between input images. In contrast, hand-crafted approaches involve the manual selection and design of specific features and fusion rules.Whether a methods is end-to-end is defined as do they require one or more handcrafted steps. Loss functions are commonly used in learning-based methods to measure the quality of the fused image and to guide the training process. Based on the loss function, the process can be classified as self-supervised, supervised or unsupervised. Finally, labeled datasets can be used to train learning-based methods, where the some form of label is available and ground-truth labels are known, or for evaluating the performance of fusion algorithms. By considering these factors, researchers can select the most appropriate image fusion algorithm for their specific application requirements.

The available VIF methods can be categorized into two groups: traditional methods, which were widely used before the advent of AI, and learning-based methods. Regardless of the classification, all methods consist of three main components: image feature extraction, fusion of features from multiple images, and reconstruction of the image from the fused features. During the feature extraction stage, features are extracted from multiple images. In the fusion stage, the extracted features are compared, and complementary features are incorporated into a single feature map or set. Finally, in the reconstruction stage, the image is reconstructed from the fused set of features. All related studies aim to improve one or more of these stages in the process.

Learning-based fusion methods are often used to combine information from multiple sources to obtain more accurate and informative results. These methods can be categorized based on whether they require ground truth labels or not. The ground truth annotations don't have to be a form of fused image. If ground truth labels are needed, they are known as supervised methods. On the other hand, if no ground truth labels are required, they are referred to as unsupervised or self-supervised methods. These fusion techniques are commonly employed in convolutional-based methods, which are widely used in image processing and computer vision applications. Regardless of the specific method used, the goal of learning-based fusion methods is to improve the quality and accuracy of the resulting output by leveraging information from multiple sources.

Learning-based image fusion algorithms are commonly categorized based on the type of network used in the algorithm. There are several types of networks that are frequently used, including CNN-based, auto-encoder based, GAN-based, transformer-based networks and others which include hybrid or handcrafted steps. Convolutional neural network (CNN)-based methods are often used due to their ability to extract features from input images and produce high-quality results. Auto-encoder-based methods are also popular, as they can effectively compress and decompress image information to obtain a fused image. Generative adversarial network (GAN)-based methods use a combination of generator and discriminator networks to produce high-quality fused images. Transformer-based methods have recently gained popularity due to their ability to process long sequences of input data efficiently. Other methods, such as sparse representation-based methods and wavelet-based methods, are also used in learning-based image fusion. Ultimately, the choice of network depends on the specific requirements of the task at hand and the available resources.

In addition to categorizing fusion algorithms based on the type of network used, another way to differentiate them is whether they are end-to-end. An end-to-end algorithm is one that can take raw input data and produce the desired output directly, without any intermediate hand-crafted steps. In the context of image fusion, end-to-end algorithms are those that can take multiple input images and produce a fused image without the need for manual feature extraction or other preprocessing steps. These algorithms are often preferred, as they can be more efficient and less prone to errors compared to non-end-to-end methods that require manual intervention. In contrast, non-end-to-end algorithms may require additional processing steps, such as registration and feature extraction, to produce the final fused image. While these steps can provide additional control over the fusion process, they can also introduce additional complexity and reduce the overall efficiency of the algorithm.

\section{Traditional Algorithms}

The traditional image fusion algorithms have been extensively studied in the literature. However, they are not without shortcomings. One of the major issues with these methods is the presence of handcrafted steps, which can lead to suboptimal results. In addition, the time complexity of some methods can be quite high, making them impractical for real-world applications.

Sparse representation (SR)\cite[]{liu2017infrared}  based methods are a popular choice for image fusion. However, they suffer from several limitations. For example, methods such as \cite{bin2016efficient} and \cite{zhang2013dictionary} require dictionary learning, which can significantly increase the time complexity of the algorithm. Furthermore, these methods include handcrafted steps, which can limit their generalizability.

Another commonly used approach for image fusion is multi-scale transformation (MST) based methods. These methods, such as \cite{hu2017adaptive} and \cite{he2017infrared}, can be quite effective at capturing various characteristics of images at different scales. However, they too suffer from limitations. One major issue is their lack of generalizability, which can make them less effective in certain scenarios.

Low-rank representation (LRR) based methods are another popular choice for image fusion. These methods, such as \cite{text}{liu2012robust}, are particularly effective at dealing with noise and other forms of image degradation. However, like the other methods, they too have limitations. For example, they may not be suitable for all types of images, particularly those with complex textures or patterns.

In summary, the success of traditional image fusion algorithms heavily relies on the quality of the feature extraction method used. While there are many different methods available, each with its own strengths and weaknesses, it is important to carefully consider the specific requirements of the problem at hand before selecting a particular method.

\section{CNN Based Deep Learning Algorithms}


First image fusion method that utilizes CNN is created by Liu et al \cite{liu2018infrared}. The paper proposed a method to fuse infrared and visible images using Convolutional Neural Networks (CNNs). The approach consisted of four main steps: preprocessing, feature extraction, fusion strategy, and reconstruction. First, the input images were resized and normalized. Then, a CNN model was trained to extract features from both the visible and infrared images. The extracted features were then fused using a weighted sum fusion strategy, and the weights were learned during training. Finally, the fused feature maps were transformed back into the image domain using a deconvolutional network. The proposed method was evaluated on various datasets and outperformed other state-of-the-art fusion methods in terms of objective quality metrics and subjective visual quality. The proposed method in the paper also utilizes a multiscale approach to fuse the source images. The images are decomposed into different scales using Laplacian and Gaussian pyramids. This allows the model to better preserve details in both the visible and infrared images during the fusion process. To train the model, high-quality images and their blurred versions are used. These blurred versions are generated by applying multiscale Gaussian filtering and random sampling, which helps the model to learn features that are robust to image blurring and noise. It's also important to note that the proposed method is a pioneering deep learning-based approach to VIF, which is a metric used to evaluate the visual quality of the fused images. By introducing CNNs to VIF, the proposed method achieved better fusion results than other state-of-the-art methods available at that time. 

CNN-based methods can be categorized into two groups: supervised and unsupervised methods. While both methods use CNNs to extract features from the input images, supervised methods require labeled data during the training process, whereas unsupervised methods do not. The majority of CNN-based image fusion methods are unsupervised, as labeled data is often difficult or expensive to obtain. However, to improve the performance of supervised methods, extra steps, such as data augmentation or transfer learning, are often employed. Despite the success of both supervised and unsupervised CNN-based methods, there is still room for improvement in image fusion techniques, particularly in scenarios where the input images have significant differences in terms of illumination, resolution, or noise.

\subsection{Supervised CNNs}

Image fusion is a challenging task, as obtaining ground truth data is often difficult or impossible. However, there are several workarounds that have been proposed to generate pseudo annotations and ground truth data. One approach is to use the results of another fusion method as ground truth data. This can be done by comparing the output of the method being evaluated to the output of a known or well-established fusion method. Another approach is to generate blurred versions of the high-quality input images and use these blurred images as labels. For example, in the study by Liu et al. \cite{liu2018infrared}, blurred versions of the high-quality input images were used to train a deep learning-based fusion model. Similarly, in the study by An et al. \cite{an2020infrared}, the results of other fusion methods were used as annotations to train a convolutional neural network for image fusion. While these approaches are not a substitute for true ground truth data, they provide a means to evaluate and improve image fusion methods, especially in scenarios where obtaining actual ground truth is challenging or impossible.

It is also wise to state that many CNN-based image fusion methods rely on transfer learning, where pre-trained models, such as ResNet50 \cite{li2019infrared,li2021infrared}, VGG19\cite{li2018infrared,ren2018infrared}, VGG16 \cite{yang2021vmdm}, and DenseNet-201 \cite{li2020unsupervised}, are used to extract features from source images. The extracted features are then processed before being fused using manually designed rules, such as weighted averaging or combining the fused base and detail parts. Some studies generate weight maps based on the extracted features \cite{li2019infrared,li2020unsupervised,yang2021vmdm}, while others optimize a loss function computed based on the extracted features using the L-BFGS method \cite{ren2018infrared}. These transfer learning-based methods provide a means to leverage pre-existing knowledge in large-scale datasets and improve the performance of image fusion methods.

\subsection{Unsupervised CNNs}