\section{Datasets}

In the image fusion task, the absence of ground truth presents a significant hurdle when it comes to developing supervised VIF methods as discussed in the Section \ref{subsec:supervised} and Section \ref{subsec:unsupervised}. The majority of deep learning-based VIF methods lean towards unsupervised approaches due to the unavailability of reliable ground truth data. However, researchers have not been deterred by this challenge and have made efforts to address it through various means. They have explored alternative methods to generate pseudo ground truth or devised innovative ways of utilizing different forms of labels to facilitate supervised training in the image fusion domain. To tackle the lack of ground truth, researchers have adopted creative strategies to generate pseudo ground truth that approximates the desired fusion result. These approaches often involve using existing fusion methods as a reference to create synthesized fused images, which can then serve as pseudo ground truth during the training process. By utilizing these pseudo ground truth images, supervised fusion methods can be trained in a manner that mimics the desired fusion outcomes, enabling the models to learn and optimize fusion performance based on the generated reference images. In addition to pseudo ground truth generation, researchers have explored alternative forms of labels that can be utilized in supervised training for image fusion. These labels may not directly correspond to ground truth fusion images but can provide valuable information and guidance during the training process. For instance, researchers have employed object masks or segmentation maps as labels, allowing the models to focus on specific regions or objects of interest during the fusion process. By leveraging such labels, supervised VIF methods can be trained to prioritize the preservation of salient features or specific visual elements in the fused images, leading to improved fusion quality and performance. In short, despite the absence of ground truth in the image fusion task, researchers have demonstrated their ingenuity by developing methods that effectively handle the challenge through the use of pseudo ground truth and alternative forms of labels. These approaches enable supervised VIF methods to leverage available information and guidance during the training process, enhancing their ability to learn and optimize fusion outcomes.

Unsupervised VIF methods employ a range of training data strategies to tackle the challenge of lacking supervision. These strategies offer diverse approaches to learn from available data and enhance the performance of image fusion techniques. One approach involves utilizing pairs of visible-infrared images as training data [37], [75], [143]. By leveraging the inherent correlation between these image modalities, the models can learn to extract and fuse relevant features effectively.

Another approach focuses on using independent visible and infrared images, which may not be directly paired, as the training data. This strategy is commonly observed in Autoencoder (AE)-based methods [28], [53], [72], [76], [78]. These methods leverage the inherent structures within individual modalities to learn meaningful representations and subsequently generate fused images. Although these training data do not possess explicit correspondences, the models aim to capture the underlying patterns and correlations between the modalities.

Additionally, some approaches make use of all-clear visible images, primarily employed in AE-based methods. For example, Li et al. [42] utilize the MSCOCO dataset to train the encoder and decoder components. By leveraging the rich visual content present in these images, the models can learn to encode and decode the relevant information necessary for fusion. This data-driven approach benefits from the availability of large-scale visible datasets, enabling the models to capture diverse visual characteristics.

Furthermore, a hybrid strategy combines visible images with visible-infrared image pairs to train different modules of the model. For instance, Jian et al. [90] employ visible images to train an image decomposition module and visible-infrared image pairs to train a stacked sparse autoencoder for local saliency map extraction. This approach leverages the complementary nature of the training data, allowing the model to learn both local saliency and decomposition features for effective fusion.

Lastly, transfer learning emerges as a viable solution to address the scarcity of training data. It involves utilizing pre-trained models that have been trained on large-scale RGB datasets, as mentioned in Section 2.4.3. By leveraging the knowledge learned from these pre-trained models, the models can acquire generic visual representations that can be adapted and fine-tuned for the task of VIF. This approach offers a practical and effective way to overcome the limitations of limited available training data.

Overall, these diverse training data strategies enable unsupervised VIF methods to learn from different sources of information and exploit inherent correlations, leading to enhanced fusion performance.

\subsection{Methods to Create Labeled Dataset}

Addressing the lack of ground truth in image fusion task is approached through various methods. One method involves using fused images generated by other techniques as a substitute for ground truth. For instance, Li et al. [24] utilize the Generative Fusion Framework (GFF) [146] to generate labels, while Lebedev et al. [22] employ the Laplacian pyramid algorithm in combination with MultiScale Retinex [144] to produce ground truth images. However, this approach may have limitations on the learning process [101].

Another approach involves utilizing clear images and their corresponding blurred versions. This technique has been applied in several studies [23], [29], [47], [56], [82], where RGB images and their blurred counterparts are utilized. Recently, Zhu et al. [112] generated blurred versions for both RGB and infrared images. Nevertheless, the training data produced using this approach may lack realism and differ from actual visible-infrared image pairs.

A different strategy involves utilizing manually-labeled object masks available in existing image fusion datasets. Some researchers [31], [94]â€“[96] use these masks to retain semantic information in the fused images. However, this method is labor-intensive and not always convenient for obtaining the required masks.

Another technique involves transforming the image fusion task, which lacks ground truth, into a task that has ground truth for a specific part of the loss function. This is achieved by incorporating labels from downstream applications. For example, Shopovska et al. [44] use a pre-trained pedestrian detector to generate pedestrian labels and include these labels as an auxiliary detection loss during training. Tang et al. [108] employ scene segmentation as a downstream task and integrate a segmentation loss term into the overall loss function to guide training. The scene segmentation labels are manually labeled by the authors of the segmentation dataset. Similarly, Liu et al. [115] utilize general object detection as a downstream task and introduce an object detection loss term to the loss function, leveraging object detection labels provided by the creator of the detection dataset.

Lastly, the Y channel of RGB images in the YCbCr color space can be used as a form of ground truth. Synthetic infrared and visible images are generated to facilitate training [102].

In summary, researchers have adopted diverse methods to overcome the absence of ground truth in image fusion tasks. These methods include using fused images from other techniques, clear images and their blurred versions, manually-labeled object masks, labels from downstream applications, or synthetic images. Each approach has its own advantages and considerations, providing alternative means to enable supervised training and enhance the performance of image fusion algorithms.

